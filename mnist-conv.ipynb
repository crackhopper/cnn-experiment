{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from six.moves import xrange\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from six.moves import urllib\n",
    "import sys\n",
    "from tensorflow.contrib import layers\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gzip\n",
    "\n",
    "####### download data ############\n",
    "\n",
    "SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\n",
    "TRAIN_IMAGES = 'train-images-idx3-ubyte.gz'\n",
    "TRAIN_LABELS = 'train-labels-idx1-ubyte.gz'\n",
    "TEST_IMAGES = 't10k-images-idx3-ubyte.gz'\n",
    "TEST_LABELS = 't10k-labels-idx1-ubyte.gz'\n",
    "datadir = os.path.abspath('./Data')\n",
    "\n",
    "if not os.path.exists(datadir):\n",
    "    os.mkdir(datadir)\n",
    "    \n",
    "if not os.path.exists(os.path.join(datadir,TRAIN_IMAGES)):\n",
    "    print('start downloading the data')\n",
    "    def processbar(filename):\n",
    "        def _process(count,block_size,total_size):\n",
    "            per = float(count*block_size)*100/total_size\n",
    "            if per > 100:\n",
    "                sys.stdout.write('\\r>>downloading {0} 100%\\n--downloaed {0}\\n'.format(filename))\n",
    "            else:\n",
    "                sys.stdout.write('\\r>>downloading %s %.1f%%'%(filename,per))          \n",
    "            sys.stdout.flush()\n",
    "        return _process\n",
    "    fname = TRAIN_IMAGES\n",
    "    surl = SOURCE_URL+fname\n",
    "    urllib.request.urlretrieve(surl, os.path.join(datadir,fname),processbar(fname))\n",
    "    \n",
    "    fname = TRAIN_LABELS\n",
    "    surl = SOURCE_URL+fname\n",
    "    urllib.request.urlretrieve(surl, os.path.join(datadir,fname),processbar(fname))\n",
    "    \n",
    "    fname = TEST_IMAGES\n",
    "    surl = SOURCE_URL+fname\n",
    "    urllib.request.urlretrieve(surl, os.path.join(datadir,fname),processbar(fname))\n",
    "\n",
    "    fname = TEST_LABELS\n",
    "    surl = SOURCE_URL+fname\n",
    "    urllib.request.urlretrieve(surl, os.path.join(datadir,fname),processbar(fname))\n",
    "\n",
    "####### unzip data ############\n",
    "\n",
    "with gzip.GzipFile(datadir+'/'+TRAIN_IMAGES) as f:\n",
    "    buf = f.read()   \n",
    "train_magic1,nimg,nrow, ncol= np.frombuffer(buf,np.dtype('>i4'),4)\n",
    "train_image = np.frombuffer(buf,np.dtype('u1'),offset=16)\n",
    "\n",
    "with gzip.GzipFile(datadir+'/'+TRAIN_LABELS) as f:\n",
    "    buf = f.read()   \n",
    "train_magic2,nlbl = np.frombuffer(buf,np.dtype('>i4'),2)\n",
    "train_label = np.frombuffer(buf,np.dtype('u1'),offset=8)\n",
    "assert(train_magic1==2051)\n",
    "assert(train_magic2==2049)\n",
    "images = [] # use python list, avoid reallocation when append elements\n",
    "for i in range(nimg):\n",
    "    img = train_image[i*nrow*ncol:(i+1)*nrow*ncol].reshape(nrow,ncol)\n",
    "    images.append(img)\n",
    "\n",
    "images = np.array(images) # change to np.array\n",
    "\n",
    "data = images.reshape([-1,28,28,1])/255.0\n",
    "label = train_label.reshape([-1,1])\n",
    "datatype = tf.float32\n",
    "print('data loaded')\n",
    "\n",
    "#####  Helper class #######\n",
    "class DSet(object):\n",
    "    def __init__(self,x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __repr__(self):\n",
    "        return 'DSet: x:%s, y:%s'%(self.x.shape,self.y.shape)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "class Dataset(object):\n",
    "    def __init__(self,data,label,per_for_test):\n",
    "        self._per_for_test = per_for_test\n",
    "        self._data = data\n",
    "        self._label = label\n",
    "        n_test = int(len(self)*self._per_for_test)\n",
    "        n_train = len(self)-n_test\n",
    "        idx_train = np.zeros(len(self),dtype=bool)\n",
    "        idx_train[np.random.choice(np.arange(len(self)),n_train,replace=False)]=True\n",
    "        \n",
    "        \n",
    "        self.train = DSet(self._data[idx_train,:],self._label[idx_train,:])\n",
    "        self.test = DSet(self._data[~idx_train,:],self._label[~idx_train,:])\n",
    "        \n",
    "        self._train_perm = np.arange(len(self.train.x))\n",
    "        \n",
    "        self._index_in_epoch = 0\n",
    "        self._epochs_completed = 0\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "    \n",
    "    # only select batch in training data\n",
    "    def next_batch(self, batch_size):\n",
    "        assert batch_size <= len(self.train)\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "        if self._index_in_epoch>len(self.train.x):\n",
    "            self._epochs_completed += 1\n",
    "            perm = np.arange(len(self.train.x))\n",
    "            self.train.x = self.train.x[perm]\n",
    "            self.train.y = self.train.y[perm]\n",
    "            start = 0\n",
    "            self._index_in_epoch=batch_size\n",
    "            assert batch_size <= len(self.train)\n",
    "        end = self._index_in_epoch\n",
    "        return DSet(self.train.x[start:end], self.train.y[start:end])\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Dataset(data=, label=, per_for_test=%f): <train:(%s-%s);  test:(%s-%s)>' % (\n",
    "            self._per_for_test,\n",
    "            str(self.train.x.shape),\n",
    "            str(self.train.y.shape),\n",
    "            str(self.test.x.shape),\n",
    "            str(self.test.y.shape),\n",
    "        )\n",
    "    \n",
    "mnist = Dataset(data,label,0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### define model ########\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def flatten(feat):\n",
    "    _shape = feat.get_shape()\n",
    "    nfeat = np.prod(_shape[1:]).value \n",
    "    with tf.variable_scope('flatten'):\n",
    "        feat = tf.reshape(feat,shape=[-1,nfeat],name='flattened')\n",
    "    return feat\n",
    "\n",
    "def one_hot(target,nlabel):\n",
    "    with tf.variable_scope('onehot'):\n",
    "        one_hot_target = tf.one_hot(tf.cast(tf.reshape(target,[-1]),tf.int32),nlabel)\n",
    "    return one_hot_target\n",
    "\n",
    "def cross_entropy_loss(logit,onehot):\n",
    "    with tf.variable_scope('cross_entropy'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logit,onehot))\n",
    "    return loss\n",
    "\n",
    "def logit_accuracy(logit,one_hot):\n",
    "    with tf.variable_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(logit, 1), tf.argmax(one_hot,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return accuracy\n",
    "\n",
    "def conv(in_tensor,in_channel,out_channel,name='conv',datatype = tf.float32):\n",
    "    with tf.variable_scope(name):\n",
    "#        h_conv1 = layers.conv2d(in_tensor,out_channel,[5,5],activation_fn=tf.nn.relu)\n",
    "#        h_pool1 = max_pool_2x2(h_conv1)\n",
    "        \n",
    "        w = tf.Variable(tf.zeros(shape=[5,5,in_channel,out_channel],dtype=datatype))\n",
    "        b = tf.Variable(tf.constant(0,shape=[out_channel],dtype=datatype))\n",
    "        conv = tf.nn.conv2d(in_tensor,w,strides=[1,1,1,1],padding='SAME')\n",
    "        z = tf.nn.bias_add(conv,b)\n",
    "        x = tf.nn.relu(z)\n",
    "        h_pool1 = max_pool_2x2(x)\n",
    "    return h_pool1\n",
    "\n",
    "def max_pool_2x2(tensor_in,name='maxpool'):\n",
    "    with tf.variable_scope(name):\n",
    "        out = tf.nn.max_pool(tensor_in,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n",
    "    return out\n",
    "\n",
    "def fc(in_tensor,n_out,name='fc',datatype = tf.float32,act=None):\n",
    "    n_in = in_tensor.get_shape()[1].value\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.Variable(layers.initializers.xavier_initializer()(shape=[n_in,n_out])) # work\n",
    "#        w = tf.Variable(tf.ones(shape=[n_in,n_out])) # terrible\n",
    "#        w = tf.Variable(tf.random_normal(shape=[n_in,n_out],stddev=0.01)) # work\n",
    "#        w = tf.Variable(tf.random_normal(shape=[n_in,n_out],stddev=0.1)) # terrifc\n",
    "#        w = tf.Variable(tf.random_normal(shape=[n_in,n_out],stddev=1)) # terrible\n",
    "        b = tf.Variable(tf.constant(0,shape=[n_out],dtype=datatype))\n",
    "        z = tf.matmul(in_tensor,w)+b\n",
    "        z = act(z) if act else z\n",
    "#        z = layers.fully_connected(in_tensor,n_out,activation_fn=act)\n",
    "    return z\n",
    "\n",
    "def conv_net(feat):\n",
    "    conv1 = conv(feat,1,32,name='conv1')\n",
    "    conv2 = conv(conv1,32,64,name='conv2')\n",
    "    flattened = flatten(conv2)\n",
    "    fc1 = fc(flattened,1024,name='fc1',act=tf.nn.relu)\n",
    "    logit = fc(fc1,10,name='fc2',act=None)\n",
    "\n",
    "    return logit\n",
    "    \n",
    "    \n",
    "    \n",
    "feat = tf.placeholder(datatype,shape=(None, 28, 28, 1))\n",
    "logits = conv_net(feat)\n",
    "\n",
    "target = tf.placeholder(datatype,shape=(None,1))\n",
    "onehot_tgt = one_hot(target,10)\n",
    "\n",
    "accuracy = logit_accuracy(logits,onehot_tgt)\n",
    "loss = cross_entropy_loss(logits,onehot_tgt)\n",
    "\n",
    "optimizer = layers.optimize_loss(loss,tf.contrib.framework.get_global_step(),\n",
    "                               optimizer='SGD',learning_rate=0.01)\n",
    "#regularizer = tf.nn.l2_loss(w)+tf.nn.l2_loss(b)\n",
    "#loss=loss+5e-4*regularizer\n",
    "#step = tf.Variable(0,dtype=datatype)\n",
    "#learning_rate = tf.train.exponential_decay(0.01,step*batchsize,N,0.95,staircase=True)\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate,0.9).minimize(loss,global_step=step)\n",
    "\n",
    "testdict = {feat: mnist.test.x[0:1000,:], target: mnist.test.y[0:1000,:]}\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in xrange(3000):\n",
    "        batch = mnist.next_batch(100)\n",
    "        traindict = {feat: batch.x, target: batch.y}\n",
    "        sess.run(optimizer,feed_dict=traindict)\n",
    "        if i%10==0:\n",
    "            print(str(i),\n",
    "                  loss.eval(feed_dict=traindict),\n",
    "                  accuracy.eval(feed_dict=testdict),\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers.fully_connected??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
